## 오늘의 목표
- 6장. 차원 축소(p.373 ~ p.404)
- 7장. 군집화(p.405 ~ p.459)


### PCA
- ratsgo님의 [주성분분석(Principal Component Analysis)
](https://ratsgo.github.io/machine%20learning/2017/04/24/PCA/)
- [차원 축소 - PCA, 주성분분석](https://excelsior-cjh.tistory.com/167?category=918734)


### LDA
- 약자가 같은 것으로 잠재디리클레할당이 있는데 다른 것임(Latent Dirichlet Allocation, LDA)


### SVD
- SVM 아님!
- 다크 프로그래머님의 [특이값 분해(Singular Value Decomposition, SVD)의 활용](https://darkpgmr.tistory.com/106)
- ratsgo님의 [SVD와 PCA, 그리고 잠재의미분석(LSA)](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/06/pcasvdlsa/)
- 최범균님의 [차원축소 훑어보기 (PCA, SVD, NMF)](https://www.slideshare.net/madvirus/pca-svd)

### 군집화
- ratsgo님의 [Clustering 개요](https://ratsgo.github.io/machine%20learning/2017/04/16/clustering/)
	- [Spectral Clustering](https://ratsgo.github.io/machine%20learning/2017/04/27/spectral/)

### 기타
- [Dim Reduction & Feature Selection](https://iostream.tistory.com/110)
- [차원 축소 - LLE](https://excelsior-cjh.tistory.com/168)

