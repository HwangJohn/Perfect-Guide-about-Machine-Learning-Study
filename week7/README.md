## 오늘의 목표
- 4장 [ LightGBM, Stacking, Imbalanced Data, Stacking ]
	- 1) LightGBM 공식 문서
		- [LightGBM Document](https://lightgbm.readthedocs.io/en/latest/)
	- 2) 참고 자료 읽기 (번역본)
	- 3) 단순 선형 회귀	

### LightGBM 참고 자료
- [LightGBM Paper](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf)
- [LightGBM 논문 번역본](https://aldente0630.github.io/data-science/2018/06/29/highly-efficient-gbdt.html)
- [CatBoost vs LightGBM vs XGBoost](https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db)

### Stacking
- [StackNet](http://blog.kaggle.com/2017/06/15/stacking-made-easy-an-introduction-to-stacknet-by-competitions-grandmaster-marios-michailidis-kazanova/)
	- [Github Repo](https://github.com/kaz-Anova/StackNet)
- [Boosting, Bagging, and Stacking - Ensemble Methods with sklearn and mlens](https://medium.com/@rrfd/boosting-bagging-and-stacking-ensemble-methods-with-sklearn-and-mlens-a455c0c982de)

### Imbalanced Data
- [비대칭 데이터 문제](https://datascienceschool.net/view-notebook/c1a8dad913f74811ae8eef5d3bedc0c3/)
- [Imbalanced data를 처리하는 기술 7가지](https://ourcstory.tistory.com/240)

### Kaggle 커널
- [Tutorial on Ensemble Learning (Don't Overfit)
](https://www.kaggle.com/mjbahmani/tutorial-on-ensemble-learning-don-t-overfit)
- [Introduction to Ensembling/Stacking in Python](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python)
