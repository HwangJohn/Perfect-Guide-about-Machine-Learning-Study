## 오늘의 목표
- 4장 [ XGBoost ]
	- 1) 앙상블 학습 ~ 랜덤 포레스트(p.206 ~ p.217)
	- 2) GBM(p.217 ~ p.222)
	- 3) XGBoost(p.222 ~ p.238)
	- 4) [XGBoost 사용하기](https://brunch.co.kr/@snobberys/137)
	- 5) [XGBoost Document](https://xgboost.readthedocs.io/en/latest/index.html)
	- 실습 
		- 파이썬 래퍼 XGBoost와 사이킷런 래퍼 XGBoost 둘 다 경험해보기
		- 공식 Documnet 천천히 읽어보기 
	

### Random Forest 참고 자료
- 이기창님의 [Random Forest, Rotation Forest](https://ratsgo.github.io/machine%20learning/2017/03/17/treeensemble/)
- [위키피디아](https://ko.wikipedia.org/wiki/%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8)
- [Random forest in python (Kaggle)](https://www.kaggle.com/raviolli77/random-forest-in-python)


### XGBoost 참고 자료
- [XGBoost Paper](https://arxiv.org/abs/1603.02754)
- [XGBoost 사용하기](https://brunch.co.kr/@snobberys/137)
- [XGBoost Document](https://xgboost.readthedocs.io/en/latest/index.html)
- [Introduction to Boosted Tress PPT](https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf?fbclid=IwAR0gGntURg4U24l6Fit-DLpVNBb_BtgMjzlSg3NYdb8jI44JLHLH-0Zluis)


### Kaggle 커널
- [Tutorial on Ensemble Learning (Don't Overfit)
](https://www.kaggle.com/mjbahmani/tutorial-on-ensemble-learning-don-t-overfit)
- [Using XGBoost with Scikit-learn](https://www.kaggle.com/stuarthallows/using-xgboost-with-scikit-learn)