## 오늘의 목표
- 5장. 회귀 실습(p.338 ~ p.372)
	- 코드 쳐보면서 연습!!!!
	- 단순히 따라치는게 아니라 꼭꼭 씹어드시길!
- 6장. 차원 축소(p.373 ~ p.404)


### 회귀 Kaggle 커널
- [Logistic regression with new features + feather
](https://www.kaggle.com/graf10a/logistic-regression-with-new-features-feather)


### PCA
- ratsgo님의 [주성분분석(Principal Component Analysis)
](https://ratsgo.github.io/machine%20learning/2017/04/24/PCA/)
- [차원 축소 - PCA, 주성분분석](https://excelsior-cjh.tistory.com/167?category=918734)


### LDA
- 약자가 같은 것으로 잠재디리클레할당이 있는데 다른 것임(Latent Dirichlet Allocation, LDA)


### SVD
- SVM 아님!
- 다크 프로그래머님의 [특이값 분해(Singular Value Decomposition, SVD)의 활용](https://darkpgmr.tistory.com/106)
- ratsgo님의 [SVD와 PCA, 그리고 잠재의미분석(LSA)](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/06/pcasvdlsa/)
- 최범균님의 [차원축소 훑어보기 (PCA, SVD, NMF)](https://www.slideshare.net/madvirus/pca-svd)


### 기타
- [Dim Reduction & Feature Selection](https://iostream.tistory.com/110)
- [차원 축소 - LLE](https://excelsior-cjh.tistory.com/168)

