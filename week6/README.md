## 오늘의 목표
- 4장 [ XGBoost, LightGBM, Stacking, Imbalanced Data, Stacking ]
	- 1) 저번 주에 공식 문서 봤나요?
		- [XGBoost Document](https://xgboost.readthedocs.io/en/latest/index.html)
		- [LightGBM Document](https://lightgbm.readthedocs.io/en/latest/)
	- 2) 참고 자료 읽기 (원한다면)
	- 3) 단순 선형 회귀
	

### XGBoost 참고 자료
- [XGBoost Paper](https://arxiv.org/abs/1603.02754)
- [XGBoost 사용하기](https://brunch.co.kr/@snobberys/137)
- [XGBoost Document](https://xgboost.readthedocs.io/en/latest/index.html)
- [Introduction to Boosted Tress PPT](https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf?fbclid=IwAR0gGntURg4U24l6Fit-DLpVNBb_BtgMjzlSg3NYdb8jI44JLHLH-0Zluis)
- [boosting 기법 이해 (bagging vs boosting)](https://www.slideshare.net/freepsw/boosting-bagging-vs-boosting)

### LightGBM 참고 자료
- [LightGBM 논문 번역](https://aldente0630.github.io/data-science/2018/06/29/highly-efficient-gbdt.html)
- [CatBoost vs LightGBM vs XGBoost](https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db)

### Gradient 
- [Gradient Boosting Algorithm의 직관적 이해](https://3months.tistory.com/368)
- [Gradient boosting performs gradient descent](https://explained.ai/gradient-boosting/descent.html)
- [Gradient Boosting(컨벡스 최적화)](https://wikidocs.net/19037)


### Kaggle 커널
- [Tutorial on Ensemble Learning (Don't Overfit)
](https://www.kaggle.com/mjbahmani/tutorial-on-ensemble-learning-don-t-overfit)
- [Using XGBoost with Scikit-learn](https://www.kaggle.com/stuarthallows/using-xgboost-with-scikit-learn)
- [Introduction to Ensembling/Stacking in Python](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python)

### Catboost
- [Catboost의 아이디어](https://ishuca.tistory.com/418)
- [Catboost 설명](https://gentlej90.tistory.com/100)

### Imbalanced Data
- [비대칭 데이터 문제](https://datascienceschool.net/view-notebook/c1a8dad913f74811ae8eef5d3bedc0c3/)
- [Imbalanced data를 처리하는 기술 7가지](https://ourcstory.tistory.com/240)